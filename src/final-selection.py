import pandas as pd
import numpy as np

INPUT_DATA_FILE = 'data/normalized.csv'
IMPORTANCE_REPORT_FILE = 'data/feature_importance.csv'
TRAIN_END_DATE = '2020-12-31' # split date
MIN_IMPORTANCE_THRESHOLD = 0.005 
MAX_CORRELATION_THRESHOLD = 0.90 # Features with an importance below 0.005 or above 0.9 will be dropped


def load_data():
    try:
        df = pd.read_csv(INPUT_DATA_FILE)
        df['Date'] = pd.to_datetime(df['Date'])
        
        # Load the report generated by the testing script
        importance_df = pd.read_csv(IMPORTANCE_REPORT_FILE, index_col=0)
        importance_df = importance_df.sort_values('RF_Importance', ascending=False)
        
        # Ensure target column is present and clean NaNs
        df = df.dropna() 
        
        return df, importance_df
    except FileNotFoundError as e:
        print(f"Error: Required file not found. Have you run the feature engineering and testing scripts? -> {e}")
        return None, None

def feature_selection(df, importance_df):
    """Filter based on importance and remove multicollinearity."""
    
    # Define features to select from (all columns except the metadata/target)
    EXCLUDE_COLS = ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 
                    'Volume', 'Dividends', 'Stock Splits', 'target_return_1m']
    
    # Filter by importance
    print(f"\n1. Filtering features below RF Importance threshold of {MIN_IMPORTANCE_THRESHOLD}...")
    
    # Get features whose RF Importance is above the threshold
    selected_features = importance_df[importance_df['RF_Importance'] > MIN_IMPORTANCE_THRESHOLD].index.tolist()
    
    # Add essential columns back for the final output
    ESSENTIAL_COLS = ['Date', 'Ticker', 'target_return_1m']
    final_cols = ESSENTIAL_COLS + [col for col in selected_features if col not in EXCLUDE_COLS]
    
    df_clean = df[final_cols].copy()
    
    print(f"   Reduced from {len(df.columns) - len(EXCLUDE_COLS)} features to {len(final_cols) - len(ESSENTIAL_COLS)}.")

    # Handle Multicollinearity (redundancy check)
    print(f"\n2. Checking for highly redundant features (> {MAX_CORRELATION_THRESHOLD * 100:.0f}% correlation)...")
    
    # Calculate correlation matrix for the remaining predictive features
    corr_matrix = df_clean.drop(columns=ESSENTIAL_COLS).corr().abs()
    
    # Iterate through the matrix to find highly correlated pairs
    features_to_drop = set()
    
    # Use the importance list to prioritize which feature to keep
    importance_rank = {col: rank for rank, col in enumerate(selected_features)}
    
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            feature1 = corr_matrix.columns[i]
            feature2 = corr_matrix.columns[j]
            
            if corr_matrix.iloc[i, j] >= MAX_CORRELATION_THRESHOLD:
                # Keep the one with the higher RF Importance score (lower rank number)
                rank1 = importance_rank.get(feature1, len(selected_features)) # Use default high rank if not found
                rank2 = importance_rank.get(feature2, len(selected_features))
                
                if rank1 < rank2:
                    features_to_drop.add(feature2) # Feature 2 is less important, so drop it
                else:
                    features_to_drop.add(feature1) # Feature 1 is less important, so drop it


    df_final = df_clean.drop(columns=list(features_to_drop), errors='ignore')
    
    print(f"   Dropped {len(features_to_drop)} redundant features.")
    print(f"   Final feature count: {len(df_final.columns) - len(ESSENTIAL_COLS)}")
    
    return df_final

def perform_time_split(df_final):
    """Split the final dataset into chronological training and testing sets."""
    
    print(f"\n3. Performing Time-Series Split...")
    
    # Split the data chronologically based on the configuration date
    train_data = df_final[df_final['Date'] <= TRAIN_END_DATE].copy()
    test_data = df_final[df_final['Date'] > TRAIN_END_DATE].copy()
    
    # Check for gaps/overlaps
    if train_data['Date'].max() >= test_data['Date'].min():
        print("⚠️ Warning: Data split has overlaps or gaps near the split date.")
        
    print(f"   Training Data Range: {train_data['Date'].min().date()} to {train_data['Date'].max().date()}")
    print(f"   Testing Data Range: {test_data['Date'].min().date()} to {test_data['Date'].max().date()}")
    print(f"   Training Rows: {len(train_data):,} | Testing Rows: {len(test_data):,}")
    
    return train_data, test_data

def save_final_datasets(train_data, test_data):
    
    # Save to CSV files
    train_output = 'data/train_data_final.csv'
    test_output = 'data/test_data_final.csv'
    
    train_data.to_csv(train_output, index=False)
    test_data.to_csv(test_output, index=False)
    
    print(f"\n4. Final Datasets Saved Successfully!")
    print(f"   - Training Set: {train_output}")
    print(f"   - Testing Set: {test_output}")
    print("\n✅ You are now ready to train your models!")


def main():
    print("=" * 70)
    print("FINAL FEATURE SELECTION & TIME SPLIT")
    print("=" * 70)
    
    # Load data and importance report
    df, importance_df = load_data()
    if df is None:
        return

    # Feature Selection (Importance filtering & Multicollinearity reduction)
    df_final = feature_selection(df, importance_df)

    # Time Split
    train_data, test_data = perform_time_split(df_final)

    # Save final files
    save_final_datasets(train_data, test_data)


if __name__ == "__main__":
    main()
